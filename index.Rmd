---
title: "Sustainability on College Campuses across the US"
author: "giRlbosses"
date: "11/18/2021"
output:
  rmdformats::readthedown:
    thumbnails: false
    highlight: "kate"
---

```{r setup, include = FALSE}
library(tidyverse)
library(kableExtra) 
library(janitor)
library(tidytext)
library(wordcloud)
library(textdata)
library(robotstxt) 
library(rvest) 
library(purrr) 
library(readxl)
library(readr)
library(fuzzyjoin)
library(readxl)
library(leaflet)

# Set code chunk defaults 
knitr::opts_chunk$set(echo = FALSE, 
                      mesage = FALSE,
                      warning = FALSE,
                      fig.align = "center")

# Set R environment options
options(knitr.kable.NA = '')
```

# Introduction

## Sustainability Report Sentiment Analysis

For the sentiment analysis section of our project, I (Gillian) scraped Amherst College's most recent sustainability report from 2017, which can be accessed [here](https://www.amherst.edu/amherst-story/today/green-amherst/reporting-and-operations/sustainability-reports/report-october-2017) through R code. I needed to be able to convert it into a data frame in R, which is why I scraped it. It is a .txt file with one string which I tokenized into words, removed the stop words, and counted the word frequencies to use in my sentiment analysis. We wanted to examine the sentiments found in this report because it will provide insight on how the college's administration discusses issues surrounding sustainability. 

```{r, message = FALSE}
#Read in sustainability report
report <- read_csv("sentimentanalysis/data/amherstsustainabilityreport.txt")

#Stop word data
data(stop_words)

#Wrangling
word_frequencies <- report %>%
  #Tokenize text into words
  unnest_tokens(output = word, input = x) %>%
  #Remove stop words
  anti_join(stop_words, by = "word") %>%
  #Word frequencies
  count(word, sort = TRUE) 
```

First, I created a common words plot that displays the top ten most common words in the report. This plot shows that investment was the most common word in the report, being written 30 times. This is interesting because it shows the institution views investment and money as very important factors when considering sustainability policy. I also created a word cloud that displays words with a minimum frequency of 5 and colors and sizes them from least to most frequent. 

```{r, warning = FALSE}
#Common words plot
word_frequencies %>%
  slice(1:10) %>%
  ggplot(aes(x = reorder(word, n), y = n, 
             color = word, fill = word)) +
  geom_col() +
  coord_flip() +
  guides(color = "none", fill = "none") +
  labs(x = NULL,
       y = "Number of instances",
       title = "The Most Common Words in Amherst's Sustainability Report")

#Word Cloud
mypal <- brewer.pal(10, "Paired")

wordcloud(words = word_frequencies$word, 
          freq = word_frequencies$n,
          min.freq = 5,
          max.words = 50,
          # plot the words in a random order
          random.order = TRUE,
          # specify the range of the size of the words
          scale = c(2, 0.3),
          # specify proportion of words with 90 degree rotation
          rot.per = 0.15,
          # colors words from least to most frequent
          colors = mypal,
          # font family
          family = "sans")
```

Next, I examined three different lexicons to try and summarize the sentiments in the report. First is the NRC Lexicon which categorizes words by emotions. I created a visualization that displays all of the emotions sorted with the lexicon and the words with the highest contribution to those sentiments. Some notable aspects from this visualization include that the fear emotion included the word change 10 times and the trust and joy emotions included a large variety of words while emotions like sadness and disgust had many less words. This provides some context about how Amherst talks about sustainability coming more from a positive place with change rather than a place of fear or distrust.

```{r, fig.height = 6}
#NRC Lexicon
nrc_lexicon <- get_sentiments("nrc")

nrc_lexicon_formatted <- inner_join(nrc_lexicon, word_frequencies, by = c("word" = "word"))

nrc_graphic <- nrc_lexicon_formatted %>%
  filter(sentiment %in% c("anger", "anticipation", "disgust", "sadness", "fear", "joy", "surprise", "trust")) %>%
  group_by(sentiment) %>%
  arrange(desc(n)) %>%
  slice(1:10) %>%
  ungroup()

nrc_graphic %>%
  ggplot(aes(x = reorder(word, n), y = n, 
             fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  coord_flip() +
  facet_wrap(~ sentiment, ncol = 2, scales = "free") +
  labs(x = NULL,
       y = "Contribution to sentiment",
       title = "The most common sentiments in Amherst's Sustainability Report, NRC")
```

Next, I created a visualization with the AFINN lexicon which looks at negative versus positive words with an integer between 5 and -5. The highest integer on this visualization is 3 with the word sucessful. However, the overall trend shows words with positive integers (positive words) with values between 1 and 2. Therefore, Amherst is talking about sustainability with generally slightly positive terms and very few negative terms. 

```{r, fig.width = 10}
#AFINN Lexicon
afinn_lexicon <- get_sentiments("afinn")

afinn_lexicon_formatted <- inner_join(afinn_lexicon, word_frequencies, by = c("word" = "word"))

ggplot(afinn_lexicon_formatted, aes(x = word, y = value, fill = n)) +
  geom_col() +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1)) +
  labs(x = NULL,
       y = "Contribution to sentiment",
       title = "The most common sentiments in Amherst's Sustainability Report, AFINN") +
        guides(fill = guide_legend(title = "Frequency"))
```

Lastly, I looked at the BING Lexicon which just sorts words into negative or positive and created a visualization that shows the negative and positive words with their frequency. Again, you can see that there are many more positive words than negative words and that the positive words seem to have higher frequencies, with the word sustainability having the highest frequency.

```{r, fig.width = 10}
#BING Lexicon
bing_words <- word_frequencies %>%
  inner_join(get_sentiments("bing")) %>%
  ungroup() 

ggplot(bing_words, aes(x = word, y = n)) +
  geom_col() +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1)) +
  facet_wrap(vars(sentiment), scales = 'free_x') +
  labs(x = NULL,
       y = "Contribution to sentiment",
       title = "The most common sentiments in Amherst's Sustainability Report, BING") 
```
The overall conclusion I drew from my sentiment analysis is that Amherst talks about sustainability from an overall positive viewpoint and tries to limit its use of negative words. There are some limitations to this conclusion. First, I only used the sustainability report Amherst published in 2017. Therefore, this is a very limited view into Amherst College's sentiments surrounding sustainability and is outdated, however, there is not a more recent report. Therefore, the population this can be applied to is very limited and the results can not be generalized beyond Amherst College within the last few years. Although the scope of this analysis is small, it is purposeful to pinpoint how the institution of our own college has recently talked about the issue of sustainability. This analysis could be extended by including all past reports, or broadening the scope to include other colleges. 

## Sustainability Degrees Awarded Across Colleges Nationwide as a Proxy for Sustainability

```{r, message = FALSE}
#Data wrangling for top 30 sustainable colleges data
library(tidyverse)
library(robotstxt) 
library(rvest) 
library(purrr) 
library(readxl)
library(readr)
library(fuzzyjoin)
library(readxl)
library(leaflet)
library(gganimate)
library(transformr)
library(gifski)
library(png)
#trying to read in my excel sheet

Top30 <- read_excel("data/Top30.xlsx")

#Can make a table of the top 30 schools using these different measurements

#reading in sustainability data
Institutions <- read_csv("data/Institutions.csv")
Tuition <- read_csv("data/Tuition Costs for Common Institutions.csv")
Degrees_by_County <- read_csv("data/Degrees Awarded by County.csv")
Growth_by_County <-  read_csv("data/Growth in Awarded Degrees.csv")
colleges <- read_csv("data/ipeds_directory_info.csv") %>%
  janitor::clean_names() %>%
  rename("institution" = "institution_name")

#working on making a map based on my data

#Getting college data

joined_uni <- left_join(Tuition, Institutions, by = c("ID University", "Year"))

View(joined_uni)
sum(is.na(joined_uni))

#cleaning up joined_uni data so only have information I need
clean_joined_uni <- joined_uni %>%
  janitor::clean_names() %>%
  select(id_university, university_x, year, state_tuition, total_graduates, out_of_state_tuition, sector, completions) %>%
rename("institution" = "university_x") 
fuzzy_uni <- stringdist_left_join(clean_joined_uni, colleges, by = "institution")
# fuzzy_uni %>%
#   filter(is.na(state)) %>%
#   distinct(institution.x)
# sum(is.na(fuzzy_uni$state))

uni_map <- fuzzy_uni %>%
  filter(!institution.x %in% c("Jefferson (Philadelphia University + Thomas Jefferson University)", "Messiah University", "Lyndon State College"))




#importing class size data
Summary_Tables <- read_excel("data/Summary Tables.xlsx")
Summary_Tables <- Summary_Tables[-c(1:4),] %>%
  janitor::clean_names() %>%
  rename("institution" = "x2", "id_university" = "national_center_for_education_statistics",
         "enrollment_type" = "x3",
         "total_enrollment" = "x4") 
  
enrollment <- Summary_Tables %>%
  select(institution, id_university, enrollment_type, total_enrollment) %>%
  #just want the undergrad
  filter(enrollment_type == "Undergraduate Enrollment", !total_enrollment == "-") %>% 
  mutate(id_university = parse_double(id_university),
         total_enrollment = parse_double(total_enrollment))

enroll_map <- left_join(uni_map, enrollment, by = "id_university") %>%
  mutate(percentdegree = (completions/(total_enrollment/4)*100))

#making map
enroll_map <- enroll_map %>%
mutate(size = case_when(out_of_state_tuition <= 20000 ~ 1,
                 (out_of_state_tuition > 20000) & (out_of_state_tuition <= 40000) ~ 2,
                 out_of_state_tuition > 40000 ~ 3)) %>%
  filter(!is.na(size)) %>%
  filter(!is.na(percentdegree))

max(enroll_map$percentdegree)
min(enroll_map$percentdegree)
# sum(is.na(uni_map$size))
pal <- colorBin(
  palette = 'Dark2',
  domain = enroll_map$percentdegree,
  bins = c(0, .1, .3, .6, .9, 1.1, 1.5, 2, 2.5, 5, 6, 10, 20, 30)
)

leaflet(data = enroll_map) %>% 
  addTiles() %>%
  addCircleMarkers(lat = ~latitude_location_of_institution_hd2019, lng = ~longitude_location_of_institution_hd2019, popup = ~ paste0("Institution: ", institution.x, "\nOut of state tuition: $", out_of_state_tuition), weight = 1,  color=~pal(percentdegree), radius = ~out_of_state_tuition/10000) %>%
  addLegend("bottomright", pal = pal, values = ~percentdegree,
    title = "Percent sustainability degree",
    labFormat = labelFormat(suffix = "%"),
    opacity = 1
  )
#creating scatterplot to investigate relationship between percent sustainability and the in state tuition

ggplot(data = enroll_map, aes(y = log(log(percentdegree + 1)), x = out_of_state_tuition)) +
  geom_point(aes(color = institution.x)) + theme(legend.position = "none") 

#dynamic histogram to see if the percent sustainability degrees differ over time
#gganimate
#first create different datasets for the different years
enroll2013 <- enroll_map %>%
  filter(year == "2013") %>%
  select(institution.x, year, percentdegree)

enroll2014 <- enroll_map %>%
  filter(year == "2014") %>%
  select(institution.x, year, percentdegree)

enroll2015 <- enroll_map %>%
  filter(year == "2015") %>%
  select(institution.x, year, percentdegree)

enroll2016 <- enroll_map %>%
  filter(year == "2016") %>%
  select(institution.x, year, percentdegree)

enroll2017 <- enroll_map %>%
  filter(year == "2017") %>%
  select(institution.x, year, percentdegree)

enroll2018 <- enroll_map %>%
  filter(year == "2018") %>%
  select(institution.x, year, percentdegree)

enroll2019 <- enroll_map %>%
  filter(year == "2019") %>%
  select(institution.x, year, percentdegree)

#make sure to adjust the size: want x axis to about 1.5 times the y axis 
anim_plot <- ggplot(enroll_map, aes(log(log(percentdegree + 1)))) + geom_density(col = "black",fill = "blue") + transition_time(year) + shadow_mark(alpha = .3) + ease_aes("linear") +
  enter_fade() +
  exit_fade()
animate(anim_plot, width = 700, height = 450)


# animate(anim_plot, fps = 10, width = 750, height = 450)
# anim_save("nations.gif")
# anim_plot %>%
#   anim_save("animation.mp4")
#transition state
# hist(uni_map$out_of_state_tuition)
#changing the radius of the different circles
  
#working on the county map
county_growth <- Growth_by_County %>%
  janitor::clean_names()
  county_growth <- county_growth %>%
  separate(id_county, into = c("test", "fips"), sep = "US") %>%
    select(fips, county, year, completions_growth)
county_degrees <- Degrees_by_County %>%
   janitor::clean_names()
  county_degrees <- county_degrees %>%
  separate(id_county, into = c("test", "fips"), sep = "US") %>%
    select(fips, county, year, completions)
#joining datasets
county_data <- left_join(county_growth,county_degrees, by = c("fips", "year"))  
#leaflet dynamic
#tigris shapefiles and fips code


```

## Header 3 (Subsubsection heading)

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For example, you can include **Bold** and _Italic_ and `Code` text.  For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

You should test out updating your GitHub Pages website:

* clone your group's blog project repo in RStudio
* update "Your Project Title Here" to a new title in the YAML header
* knit `index.Rmd` (we will now knit to HTML by default instead of pdf)
* commit and push **both** `index.Rmd` and `index.html`
* go to https://stat231-f21.github.io/blog_giRlbosses/ to see the published test document (this is publicly available!)

## Including code and plots

You can embed code as normal, for example:

```{r cars}
summary(cars)
```

Let's clean up the format of that output instead of using the standard R output:

```{r pretty-table, echo = TRUE}
summary(cars) %>%
  kable(col.names = c("Speed", "Distance"),
        row.names = FALSE) %>%
  kable_styling(bootstrap_options = "striped",
                full_width = FALSE) %>%
  row_spec(0, bold = TRUE) %>%
  column_spec(1:2, width = "1.5in") 
```

In a study from the 1920s, fifty cars were used to see how the speed of the car and the distance taken to stop were related.  Speeds ranged between `r min(cars$speed)` and `r max(cars$speed)` mph.  Distances taken to stop ranged between `r min(cars$dist)` and `r max(cars$dist)` feet, with the middle 50% falling between `r as.numeric(quantile(cars$dist)[2])` and `r as.numeric(quantile(cars$dist)[4])` feet.  

You can also embed plots as normal, for example:

```{r figure1}
ggplot(data = cars, aes(x = speed, y = dist)) + 
  geom_point() + 
  labs(x = "Speed of car (mph)",
       y = "Distance taken to stop (ft)",
       title = "Stopping distance increases with faster speeds",
       subtitle = "Based on 1920s study") +
  theme_classic()
```

Take note of the default code chunk options in the `setup` code chunk, and adjust individual code chunk options as needed. for example, unlike the rest of the Rmd files we worked in this semester, the default code chunk option is `echo = FALSE`, so you will need to set `echo  = TRUE` for any code chunks you would like to display in the blog. 


## Including links and images/videos

You can include [links](https://www.datadreaming.org/post/r-markdown-theme-gallery/) and there are a few ways to embed  images! Both options for embedding images below can be used interchangeably. They both work for png, pdf, jpg, and even gif formats, and both support filepaths that are either URLs (for videos, you can include links to any valid YouTube or Vimeo URLs; see [here](https://bookdown.org/yihui/rmarkdown/learnr-videos.html) for more details) or point to a location within your project directory. 

### Option 1: Markdown approach

![This is a figure caption. The artwork is called Safe Space by  [Kenesha Sneed](https://www.keneshasneed.com/#/safespace/)](img/Kenesha-Sneed_safe-space.jpeg)


### Option 2: Code chunk approach

```{r, fig.cap = "This is also figure caption"}
knitr::include_graphics("https://media.giphy.com/media/H7ZrrA9V2pd3Tehdds/giphy.gif")
```

## Including equations

Equations may be needed if you are explaining a new technique or perhaps providing some other relevant formulas in your exposition. There are two ways to include equations:

* Inline: $b \sim N(0, \sigma^2_b)$
* Display-style (displayed on its own line): $$\frac{\sigma^2_b}{\sigma^2_b + \sigma^2_e}$$

For typesetting equations appropriately, take a look at the *Symbols in math mode* section of this  [cheat sheet](https://users.dickinson.edu/~richesod/latex/latexcheatsheet.pdf)  (or do some extra Googling---there are *many* resources).


# You can even create tabs within your webpage if you want! {.tabset .tabset-fade .tabset-pills}

Every subsection heading (starting with `##`) until you create a new section heading (starting with `#`) will be a new tab.

## Bulleted list

You can make a bulleted list like this:

* item 1
* item 2
* item 3


## Numbered list

You can make a numbered list like this

1. First thing I want to say
2. Second thing I want to say
3. Third thing I want to say

# Customizing your blog design

As a *final* detail **only** if you have time, you can explore options for customizing the style of your blog. By default, we are using the `readthedown` theme from the [**rmdformats** package](https://github.com/juba/rmdformats) (see Line 6 of this file if you want to switch out themes). There are, I'm sure, many many many more similar packages with built in themes, or you can look into how to include a CSS code chunk to customize aspects of the current theme.  

There are some easy-to-change options that you can play around with:

* The theme itself (Line 6): `rmdformats::readthedown`, `rmdformats::downcute`, `rmdformats::robobook`, `rmdformats::material`,
  * For `downcute` only, you can add a new indented line below Line 6 with the code `downcute_theme: "chaos"` for the `downcute chaos` theme
  * I would *not* recommend the other themes that do not have the sidebar navigation
  
* Syntax highlighting options (Line 8, `highlight`): `"default"`, `"tango"`, `"pygments"`, `"kate"`, `"monochrome"`, `"espresso"`, `"zenburn"`, `"haddock"`, or `"textmate"` (or `NULL` for no syntax highlighting)


You can explore additional customizable YAML options by looking at the [**rmdformats** package](https://github.com/juba/rmdformats) page or running, for example, `?rmdformats::readthedown()` to see the help documentation for a particular theme from the package.
